import argparse
from argparse import RawTextHelpFormatter
import os
import torch

from model_utils import *
from spec_dec_utils import * 
from compress_layers import * 
from prune import * 
from quantize import * 

def build_draft_model(target_model, tokenizer, args): 
    if args.compression_method == 'compress_layers': 
        selected_blocks = calc_transformer_blocks(target_model, args.keep_layers)
        draft_model = compress_model(target_model, args.model_name,selected_blocks)
        #draft_model.cuda()
    elif args.compression_method == 'prune': 
        draft_model = apply_topk_pruning(target_model, args.prune_method, args.density, args.structured, block_size=args.block_size)
        #draft_model.cuda()
    elif args.compression_method == 'quantize': 
        draft_model = quantize(target_model, args.quantization_method, tokenizer, args.bit_precision)
        #if args.quantization_method != 'NF4': 
            #draft_model.cuda()
    
    return draft_model

def gen_text(model, tokenizer, prompt, max_length): 
    # Use model to generate text 
    input_ids = tokenizer.encode(prompt, return_tensors="pt").cuda() 
    output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    for output_id in output_ids:
        output_text = tokenizer.decode(output_id, skip_special_tokens=True)
        print(output_text)

def main(args): 
    os.environ['CUDA_VISIBLE_DEVICES'] = args.device_number
    torch.manual_seed(args.seed)

    # Initializes model 
    target_model, tokenizer = build_model(args.model_name)
    #target_model.cuda() # Use GPU 

    draft_model = build_draft_model(target_model, tokenizer, args)
    tokens = speculative_decoding(target_model, draft_model, tokenizer, args.prompt, args.max_length, args.num_tokens)
    for token in tokens: 
        output_text = tokenizer.decode(token, skip_special_tokens=True)
        print(output_text)

if __name__ == '__main__': 
    parser = argparse.ArgumentParser(description='PEFT Methods for LLMs', formatter_class=RawTextHelpFormatter)

    # System params
    parser.add_argument('--device-number', type=str, default='0',
        help='GPU device number to use for training (-1 if using CPU training)')
    parser.add_argument('--seed', type=int, default=0,
        help='Torch random seed')
    parser.add_argument('--verbose', type=int, default=0,
        help='Print output')

    # Model params
    parser.add_argument('--model-name', type=str, default='GPT2',
        help='Which model to use')
    
    # Compression params
    parser.add_argument('--compression-method', type=str, default='prune',
        help='Which method for model compression to apply')
    
    # Pruning params 
    parser.add_argument('--density', type=float, default=0.25,
        help='Sparsity ratio for pruning')
    parser.add_argument('--prune-method', type=str, default='l1', 
        help='Scoring function to use for pruning')
    parser.add_argument('--structured', type=int, default=1, 
        help='Use structured vs. unstructured pruning')
    parser.add_argument('--block-size', type=int, default=5, 
        help='Size of blocks to use when performing structured pruning')

    # Quantization params
    parser.add_argument('--quantization-method', type=str, default='GPTQ',
        help='Which quantization method to use')
    parser.add_argument('--bit-precision', type=int, default=4, 
        help='Number of bits to keep in quantization')
    
    # Layer compression params params
    parser.add_argument('--keep-layers', type=int, default=3, 
        help='How many transformer blocks from the original transformer to keep')
    
    # Prompt params
    parser.add_argument('--prompt', type=str, default='The quick brown fox',
        help='Input prompt')
    parser.add_argument('--max-length', type=int, default=20,
        help='Max output length')
    parser.add_argument('--num-tokens', type=int, default=5,
        help='Number of tokens that should be generated by the draft model')
    
    args = parser.parse_args()

    main(args)