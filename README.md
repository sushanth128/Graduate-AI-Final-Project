# Graduate-AI-Final-Project

DRAFT MODEL GENERATION FOR SPECULATIVE DECODING

Arian Raje, Sushanth Gangireddy, Victor Akinwande

August 25, 2024

ABSTRACT

Autoregressive decoding using Large Language Models (LLMs) is slow as only a single token is generated at a time. To generate a sequence of n tokens, the model must be run n times with each generated token appended onto the prompt to generate the next token in the sequence. Speculative decoding offers an alternative approach to inference where a smaller and more efficient draft model is used to generate tokens autoregressively. Then the original LLM, which is referred to as the target model, verifies the tokens generated by the draft model in parallel, accepting and rejecting a portion of them. However, most speculative decoding research either assumes that a draft model with the same vocabulary and training data already exists for a given target model or focuses entirely on generating an effective draft model for a specific target model of interest. Our project focuses on draft model construction for an arbitrary target model. We use traditional model compression techniques and evaluate their effectiveness in a speculative decoding inference framework. We find that choosing the method that best balances model size and model utility leads to consistent speedups in inference.


Please refer to the PDF for our entire project. Thank you!
